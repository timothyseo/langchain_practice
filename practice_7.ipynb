{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트를 숫자로 바꾸는 텍스트 임베딩   \n",
    "    RAG의 기본 작동 과정은 아래와 같다.   \n",
    "    # Document(문서) -> Chunk(글 덩어리) -> Vector Store -- 질문 임베딩과 고 유사도 Vector 청크 프롬프트 투입-- -> LLM(토큰 제한)   \n",
    "    이런 작동 단계에서 임베딩은 1. 문서 청크를 벡터 DB에 저장할 때 2. 사용자의 질문에 답할 근거를 벡터 DB에서 검색할 때 사용된다.   \n",
    "    - 사용자 질문에 대한 근거를 문서 내 키워드 검색을 통해서 수행할 수 없나?   \n",
    "      - 가능하지만 고려해야 할 제약이 많다.   \n",
    "        1. 키워드는 무엇인가   \n",
    "        2. 사용자 질문이 키워드 몇 개로 대표되나   \n",
    "        3. 질문 속에 근거를 찾기 위한 키워드가 포함되어 있나   \n",
    "    - 문서를 분할한 청크를 임베딩으로 변환하여 사용자 질문과 높은 유사도를 지닌 청크를 찾는게 더 효율적   \n",
    "\n",
    "    임베딩 모델이란?\n",
    "        - 텍스트를 수치로 변환하는 작업이 임베딩 / 대량의 텍스트로 사전 학습된 모델을 활용함 / 대표 모델은 BERT   \n",
    "        - 대량의 텍스트 문서를 레이블링 하지 않고 encoder 구조를 활용해 사전학습함으로 Sentence-BERT모델은 지정된 max_token값 범위 안에서 문장을 벡터화하여 학습   \n",
    "        - 단어와 문장의 맥락 정보까지 모델이 파악해서 I ate an apple이라는 문장 속의 apple이 스마트폰 회사나 스마트폰이 아니라는것도 파악 가능 / 이런 패턴과 맥락을 수치로 나타냄   \n",
    "    # Open model은 허깅페이스에서 무료로 사용할 수 있고 Closed model은 기업의 모델로 API를 호출해서 사용한다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 768)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OpenAI 임베딩 모델 대신 구글 임베딩 모델로 오픈소스 모델 활용 예제 \n",
    "\n",
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "api_key = os.environ[\"GEMINI_API_KEY\"]\n",
    "\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=api_key)\n",
    "embeddings = embedding_model.embed_documents(\n",
    "    [\n",
    "        \"Hi there!\",\n",
    "        \"Oh, hello!\",\n",
    "        \"What's your name?\",\n",
    "        \"My friends call me World.\",\n",
    "        \"Hello, World!\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI 임베딩 모델은 한 문장에 대한 임베딩 값을 1536개 벡터로 변환 / 구글임베딩 모델은 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 768)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFium2Loader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 임베딩 모델 API 호출\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\", google_api_key=api_key)\n",
    "\n",
    "# PDF 문서 로드\n",
    "loader = PyPDFium2Loader(\"./data/[이슈리포트 2022-2호] 혁신성장 정책금융 동향.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "# PDF문서를 여러 청크로 분할\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(pages)\n",
    "\n",
    "# 임베딩 모델로 청크들 임베딩 변환\n",
    "embeddings = embedding_model.embed_documents([i.page_content for i in texts])\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 벡터로 바꾸고 각 벡터 간의 거리를 측정해서 거리가 가까운 순서대로 유사도를 계산한다.    \n",
    "임베딩 간의 거리를 구할 때 코사인 유사도를 활용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9430132914597402\n",
      "0.7049958828721193\n",
      "0.6887223978447613\n"
     ]
    }
   ],
   "source": [
    "# 텍스트들 임베딩 벡터 간 유사도 측정\n",
    "\n",
    "examples = embedding_model.embed_documents(\n",
    "    [\n",
    "        \"안녕하세요.\",\n",
    "        \"제 이름은 홍두깨입니다.\",\n",
    "        \"이름이 무엇인가요?\",\n",
    "        \"랭체인은 유용합니다.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 예시 질문과 답변 임베딩 \n",
    "embedded_query_q = embedding_model.embed_query(\"이 대화에서 언급된 이름은 무엇입니까?\")\n",
    "embedded_query_a = embedding_model.embed_query(\"이 대화에서 언급된 이름은 홍길동입니다.\")\n",
    "\n",
    "# 대화에서 언급된 이름에 대한 질문에 가장 적절한 답을 찾아야 하는 상황 가정\n",
    "# 예시 답변으로 주어진 답변 문장과 질문 유사도가 가장 높은 문장은 두번째와 세번째 순으로 유사도가 높게 나올 것으로 예상\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm \n",
    "import numpy as np \n",
    " \n",
    "def cos_sim(A, B):\n",
    "    return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "print(cos_sim(embedded_query_q, embedded_query_a))\n",
    "print(cos_sim(embedded_query_a, examples[1]))\n",
    "print(cos_sim(embedded_query_a, examples[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8385674802306704\n",
      "0.6190615117812432\n",
      "0.5094754342820353\n"
     ]
    }
   ],
   "source": [
    "# 텍스트들 임베딩 벡터 간 유사도 측정\n",
    "\n",
    "examples = embedding_model.embed_documents(\n",
    "    [\n",
    "        \"Hi.\",\n",
    "        \"My name is Tim.\",\n",
    "        \"What's your name?\",\n",
    "        \"Langchain is useful.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 예시 질문과 답변 임베딩 \n",
    "embedded_query_q = embedding_model.embed_query(\"What's the name mentioned in the conversation?\")\n",
    "embedded_query_a = embedding_model.embed_query(\"The name mentioned in the conversation is Ted.\")\n",
    "\n",
    "# 대화에서 언급된 이름에 대한 질문에 가장 적절한 답을 찾아야 하는 상황 가정\n",
    "# 예시 답변으로 주어진 답변 문장과 질문 유사도가 가장 높은 문장은 두번째와 세번째 순으로 유사도가 높게 나올 것으로 예상\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm \n",
    "import numpy as np \n",
    " \n",
    "def cos_sim(A, B):\n",
    "    return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "print(cos_sim(embedded_query_q, embedded_query_a))\n",
    "print(cos_sim(embedded_query_a, examples[1]))\n",
    "print(cos_sim(embedded_query_a, examples[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6070005807182443\n",
      "0.3590359241854671\n",
      "0.2546047637215995\n"
     ]
    }
   ],
   "source": [
    "# Open source 임베딩 모델 유사도 측정 \n",
    "# 1. jhgan/ko-sroberta-multitask 임베딩 모델\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# HuggingFaceEmbedding 함수로 오픈 소스 임베딩 모델 로드\n",
    "model_name = \"jhgan/ko-sroberta-multitask\"\n",
    "ko_embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "examples = ko_embeddings.embed_documents(\n",
    "    [\n",
    "        \"안녕하세요.\",\n",
    "        \"제 이름은 홍두깨입니다.\",\n",
    "        \"이름이 무엇인가요?\",\n",
    "        \"랭체인은 유용합니다.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "embedded_query_q = ko_embeddings.embed_query(\"이 대화에서 언급된 이름은 무엇입니까?\")\n",
    "embedded_query_a = ko_embeddings.embed_query(\"이 대화에서 언급된 이름은 홍길동입니다.\")\n",
    "\n",
    "print(cos_sim(embedded_query_q, embedded_query_a))\n",
    "print(cos_sim(embedded_query_a, examples[1]))           \n",
    "print(cos_sim(embedded_query_a, examples[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9554541293280951\n",
      "0.9322697037415678\n",
      "0.9105264657723084\n"
     ]
    }
   ],
   "source": [
    "# 2. BAAI/bge-small-en 임베딩 모델 활용\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "bge_embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "\n",
    "examples = bge_embeddings.embed_documents(\n",
    "    [\n",
    "        \"안녕하세요.\",\n",
    "        \"제 이름은 홍두깨입니다.\",\n",
    "        \"이름이 무엇인가요?\",\n",
    "        \"랭체인은 유용합니다.\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "embedded_query_q = bge_embeddings.embed_query(\"이 대화에서 언급된 이름은 무엇입니까?\")\n",
    "embedded_query_a = bge_embeddings.embed_query(\"이 대화에서 언급된 이름은 홍길동입니다.\")\n",
    "\n",
    "print(cos_sim(embedded_query_q, embedded_query_a))\n",
    "print(cos_sim(embedded_query_a, examples[1]))\n",
    "print(cos_sim(embedded_query_a, examples[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유사도 측정 시 1번 모델에 비해 2번 모델의 값이 크다.    \n",
    "이유는 해당 임베딩 모델은 사전 학습 시 영어와 중국어에 대해서만 학습되었기 때문에 다국어 모델인 text-embedding-3-small에 비해 한글 문장 임베딩 성능이 떨어진다.   \n",
    "이처럼 임베딩 모델은 사전 학습 시 어떤 언어 데이터셋으로 학습되었는지도 특정 언어의 문장 임베딩 성능에 영향을 준다.   \n",
    "RAG의 청크 임베딩 과정에서 어떤 언어를 사용하는지 고려하여 적절한 임베딩 모델을 사용하는 것이 중요하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4005386104557453\n",
      "0.4085441476443125\n",
      "0.20658353143916394\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"This is the first draft.\",\n",
    "    \"This is the second one.\",\n",
    "    \"Where is the third one?\",\n",
    "    \"Is this the first draft?\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(corpus)\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm \n",
    "\n",
    "def cos_sim(A, B):\n",
    "    A = A.toarray().flatten()\n",
    "    B = B.toarray().flatten()\n",
    "    return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "print(cos_sim(vectors[0], vectors[1]))\n",
    "print(cos_sim(vectors[1], vectors[2]))\n",
    "print(cos_sim(vectors[0], vectors[2]))\n",
    "print(cos_sim(vectors[0], vectors[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40053861045574535\n",
      "0.4085441476443126\n",
      "0.20658353143916397\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "corpus = [\n",
    "    \"This is the first draft.\",\n",
    "    \"This is the second one.\",\n",
    "    \"Where is the third one?\",\n",
    "    \"Is this the first draft?\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectors = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(cosine_similarity(vectors[0], vectors[1])[0][0])\n",
    "print(cosine_similarity(vectors[1], vectors[2])[0][0])\n",
    "print(cosine_similarity(vectors[0], vectors[2])[0][0])\n",
    "print(cosine_similarity(vectors[0], vectors[3])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def get_sentence_embeddings(sentences, model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'):\n",
    "    # 토크나이저와 모델 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    # 연산 중에 경사 계산을 비활성화\n",
    "    embeddings = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sentence in sentences:\n",
    "            # 토큰화 및 모델 입력 준비\n",
    "            inputs = tokenizer(sentence, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            # 모델을 통한 임베딩 계산\n",
    "            outputs = model(**inputs)\n",
    "            # 마지막 은닉 상태 사용\n",
    "            embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "    \n",
    "    return np.array(embeddings)\n",
    "\n",
    "def compute_similarity_matrix(sentences):\n",
    "    # 문장 임베딩 구하기\n",
    "    embeddings = get_sentence_embeddings(sentences)\n",
    "    # 유사도 행렬 계산\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "def compare_sentences(sentence1, sentence2):\n",
    "    sentences = [sentence1, sentence2]\n",
    "    similarity_matrix = compute_similarity_matrix(sentences)\n",
    "    \n",
    "    return similarity_matrix[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 1: '나는 학교에 갑니다'\n",
      "문장 2: '학교에 나는 갑니다'\n",
      "유사도: 0.9965\n",
      "\n",
      "문장 1: '나는 학교에 갑니다'\n",
      "문장 2: '나는 학교로 향합니다'\n",
      "유사도: 0.9121\n",
      "\n",
      "문장 1: '나는 학교에 갑니다'\n",
      "문장 2: '학교는 집에서 멀리 있습니다'\n",
      "유사도: 0.4730\n",
      "\n",
      "문장 1: '학교에 나는 갑니다'\n",
      "문장 2: '나는 학교로 향합니다'\n",
      "유사도: 0.8995\n",
      "\n",
      "문장 1: '학교에 나는 갑니다'\n",
      "문장 2: '학교는 집에서 멀리 있습니다'\n",
      "유사도: 0.4709\n",
      "\n",
      "문장 1: '나는 학교로 향합니다'\n",
      "문장 2: '학교는 집에서 멀리 있습니다'\n",
      "유사도: 0.5149\n",
      "\n",
      "전체 유사도 행렬:\n",
      "[[0.99999964 0.9965155  0.91214895 0.47301492]\n",
      " [0.9965155  0.99999994 0.89945614 0.47087014]\n",
      " [0.91214895 0.89945614 0.9999998  0.51485926]\n",
      " [0.47301492 0.47087014 0.51485926 0.99999994]]\n",
      "\n",
      "단어 순서가 중요한 문장들의 유사도 행렬:\n",
      "0: 영희가 철수를 좋아한다\n",
      "1: 철수가 영희를 좋아한다\n",
      "2: 영희는 철수에게 선물을 주었다\n",
      "3: 철수는 영희에게 선물을 주었다\n",
      "[[1.         0.9836133  0.9531579  0.96602637]\n",
      " [0.9836133  1.0000004  0.94147426 0.96708775]\n",
      " [0.9531579  0.94147426 1.         0.98728764]\n",
      " [0.96602637 0.96708775 0.98728764 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 예제 사용\n",
    "\n",
    "# 예제 문장들\n",
    "sentences = [\n",
    "    \"나는 학교에 갑니다\",\n",
    "    \"학교에 나는 갑니다\",  # 단어 순서만 바뀐 문장\n",
    "    \"나는 학교로 향합니다\",  # 유사한 의미를 가진 문장\n",
    "    \"학교는 집에서 멀리 있습니다\"  # 관련은 있지만 다른 의미의 문장\n",
    "]\n",
    "    \n",
    "# 문장 쌍 간의 유사도 계산 및 출력\n",
    "for i in range(len(sentences)):\n",
    "    for j in range(i+1, len(sentences)):\n",
    "        similarity = compare_sentences(sentences[i], sentences[j])\n",
    "        print(f\"문장 1: '{sentences[i]}'\")\n",
    "        print(f\"문장 2: '{sentences[j]}'\")\n",
    "        print(f\"유사도: {similarity:.4f}\\n\")\n",
    "            \n",
    "# 전체 유사도 행렬 계산\n",
    "similarity_matrix = compute_similarity_matrix(sentences)\n",
    "print(\"전체 유사도 행렬:\")\n",
    "print(similarity_matrix)\n",
    "    \n",
    "# 단어 순서가 중요한 예제\n",
    "order_examples = [\n",
    "    \"영희가 철수를 좋아한다\",\n",
    "    \"철수가 영희를 좋아한다\",  # 단어 순서가 바뀌어 의미가 달라짐\n",
    "    \"영희는 철수에게 선물을 주었다\",\n",
    "    \"철수는 영희에게 선물을 주었다\"  # 단어 순서가 바뀌어 의미가 달라짐\n",
    "]\n",
    "    \n",
    "# 단어 순서가 중요한 예제의 유사도 행렬\n",
    "order_matrix = compute_similarity_matrix(order_examples)\n",
    "print(\"\\n단어 순서가 중요한 문장들의 유사도 행렬:\")\n",
    "for i, sentence in enumerate(order_examples):\n",
    "    print(f\"{i}: {sentence}\")\n",
    "print(order_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
